"""Core agent for PR review using OpenHands SDK."""

import logging
import subprocess
from pathlib import Path
from typing import Any, Literal
from urllib.parse import urlparse

from . import _tty as _terminal_safety  # noqa: F401

from dotenv import load_dotenv
from openhands.sdk import Conversation
from openhands.sdk.conversation import get_agent_final_response
from openhands.sdk.event import Event
from openhands.sdk.workspace import LocalWorkspace
from openhands.tools.delegate.visualizer import DelegationVisualizer

from .github import GitHubAPIError, fetch_github_pr_info, normalize_github_metadata
from .gitlab import GitLabAPIError, fetch_gitlab_mr_info, post_gitlab_mr_comment
from .llm import create_hodor_agent
from .prompts.pr_review_prompt import build_pr_review_prompt
from .skills import discover_skills
from .workspace import cleanup_workspace, setup_workspace

# Load environment variables
load_dotenv()

# Configure logging
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

Platform = Literal["github", "gitlab"]


def detect_platform(pr_url: str) -> Platform:
    """Detect the platform (GitHub or GitLab) from the PR URL."""
    parsed = urlparse(pr_url)
    hostname = parsed.hostname or ""

    # Check for GitLab-specific patterns first (works for both gitlab.com and self-hosted)
    if "/-/merge_requests/" in pr_url or "gitlab" in hostname:
        return "gitlab"
    # Check for GitHub-specific patterns
    elif "/pull/" in pr_url or "github" in hostname:
        return "github"
    else:
        logger.debug(f"Unknown platform for URL {pr_url}, defaulting to GitHub")
        return "github"


def parse_pr_url(pr_url: str) -> tuple[str, str, int, str]:
    """
    Parse PR/MR URL to extract owner, repo, PR/MR number, and host.

    Examples:
        GitHub: https://github.com/owner/repo/pull/123 -> ('owner', 'repo', 123, 'github.com')
        GitLab: https://gitlab.com/owner/repo/-/merge_requests/123 -> ('owner', 'repo', 123, 'gitlab.com')
        Self-hosted: https://gitlab.example.com/group/repo/-/merge_requests/118 -> ('group', 'repo', 118, 'gitlab.example.com')
    """
    parsed = urlparse(pr_url)
    path_parts = [p for p in parsed.path.split("/") if p]
    host = parsed.netloc

    # GitHub format: /owner/repo/pull/123
    if len(path_parts) >= 4 and path_parts[2] == "pull":
        owner = path_parts[0]
        repo = path_parts[1]
        pr_number = int(path_parts[3])
        return owner, repo, pr_number, host

    # GitLab format: /group/subgroup/repo/-/merge_requests/123
    elif "merge_requests" in path_parts:
        mr_index = path_parts.index("merge_requests")
        if mr_index < 2 or mr_index + 1 >= len(path_parts):
            raise ValueError(f"Invalid GitLab MR URL format: {pr_url}. Expected .../-/merge_requests/<number>")
        if path_parts[mr_index - 1] != "-":
            raise ValueError(f"Invalid GitLab MR URL format: {pr_url}. Missing '/-/' segment before merge_requests.")

        repo = path_parts[mr_index - 2]
        owner_parts = path_parts[: mr_index - 2]
        owner = "/".join(owner_parts) if owner_parts else path_parts[0]
        pr_number = int(path_parts[mr_index + 1])
        return owner, repo, pr_number, host

    else:
        raise ValueError(
            f"Invalid PR/MR URL format: {pr_url}. Expected GitHub pull request or GitLab merge request URL."
        )


def post_review_comment(
    pr_url: str,
    review_text: str,
    model: str | None = None,
) -> dict[str, Any]:
    """
    Post a review comment on a GitHub PR or GitLab MR using CLI tools.

    Args:
        pr_url: URL of the pull request or merge request
        review_text: The review text to post as a comment
        model: LLM model used for the review (optional, for transparency)

    Returns:
        Dictionary with comment posting result
    """
    platform = detect_platform(pr_url)
    logger.info(f"Posting comment to {platform} PR/MR: {pr_url}")

    try:
        owner, repo, pr_number, host = parse_pr_url(pr_url)
    except ValueError as e:
        return {"success": False, "error": str(e)}

    # Append model information to review text for transparency
    if model:
        review_text_with_footer = f"{review_text}\n\n---\n\n*Review generated by Hodor using `{model}`*"
    else:
        review_text_with_footer = review_text

    try:
        if platform == "github":
            # Use gh CLI to post comment
            subprocess.run(
                [
                    "gh",
                    "pr",
                    "review",
                    str(pr_number),
                    "--repo",
                    f"{owner}/{repo}",
                    "--comment",
                    "--body",
                    review_text_with_footer,
                ],
                check=True,
                capture_output=True,
                text=True,
            )
            logger.info(f"Successfully posted review to GitHub PR #{pr_number}")
            return {"success": True, "platform": "github", "pr_number": pr_number}

        elif platform == "gitlab":
            # Use glab CLI to post comment
            post_gitlab_mr_comment(
                owner,
                repo,
                pr_number,
                review_text_with_footer,
                host=host,
            )
            logger.info(f"Successfully posted review to GitLab MR !{pr_number} on {owner}/{repo}")
            return {"success": True, "platform": "gitlab", "mr_number": pr_number}

        else:
            return {"success": False, "error": f"Unsupported platform: {platform}"}

    except GitLabAPIError as e:
        logger.error(f"Failed to post GitLab comment: {e}")
        return {"success": False, "error": str(e)}
    except subprocess.CalledProcessError as e:
        logger.error(f"Failed to post GitHub comment: {e}")
        logger.error(f"Command output: {e.stderr if hasattr(e, 'stderr') else 'N/A'}")
        return {"success": False, "error": str(e)}
    except Exception as e:
        logger.error(f"Error posting comment: {str(e)}")
        return {"success": False, "error": str(e)}


def review_pr(
    pr_url: str,
    model: str = "anthropic/claude-sonnet-4-5-20250929",
    temperature: float | None = None,
    reasoning_effort: str | None = None,
    custom_prompt: str | None = None,
    prompt_file: Path | None = None,
    user_llm_params: dict[str, Any] | None = None,
    verbose: bool = False,
    cleanup: bool = True,
    workspace_dir: Path | None = None,
    output_format: str = "markdown",
    max_iterations: int = 500,
    lite_model: str | None = None,
    enable_subagents: bool = True,
) -> str:
    """
    Review a pull request using OpenHands agent with bash tools.

    Args:
        pr_url: URL of the pull request or merge request
        model: LLM model name (default: Claude Sonnet 4.5)
        temperature: Sampling temperature (if None, auto-selected)
        reasoning_effort: For reasoning models: "low", "medium", or "high"
        custom_prompt: Optional custom prompt text (inline)
        prompt_file: Optional path to custom prompt file
        user_llm_params: Additional LLM parameters
        verbose: Enable verbose logging
        cleanup: Clean up workspace after review (default: True)
        workspace_dir: Directory to use for workspace (if None, creates temp dir). Reuses if same repo.
        output_format: Output format - "markdown" or "json" (default: "markdown")
        max_iterations: Maximum number of agent iterations (default: 500, use -1 for unlimited)
        lite_model: Lite model for worker subagents (e.g., "anthropic/claude-3-5-haiku-20241022")
        enable_subagents: Enable worker subagent delegation (default: True)

    Returns:
        Review text as string (format depends on output_format)

    Raises:
        ValueError: If URL is invalid
        RuntimeError: If review fails
    """
    logger.info(f"Starting PR review for: {pr_url}")

    # Parse PR URL
    try:
        owner, repo, pr_number, host = parse_pr_url(pr_url)
        platform = detect_platform(pr_url)
    except ValueError as e:
        logger.error(f"Invalid PR URL: {e}")
        raise

    logger.info(f"Platform: {platform}, Repo: {owner}/{repo}, PR: {pr_number}, Host: {host}")

    # Setup workspace (clone repo and checkout PR branch)
    workspace = None
    target_branch = "main"  # Default fallback
    diff_base_sha = None  # GitLab CI provides this for deterministic diffs
    try:
        workspace, target_branch, diff_base_sha = setup_workspace(
            platform=platform,
            owner=owner,
            repo=repo,
            pr_number=str(pr_number),
            host=host,
            working_dir=workspace_dir,
            reuse=workspace_dir is not None,  # Only reuse if user specified a workspace dir
        )
        logger.info(
            f"Workspace ready: {workspace} (target branch: {target_branch}, "
            f"diff_base_sha: {diff_base_sha[:8] if diff_base_sha else 'N/A'})"
        )
    except Exception as e:
        logger.error(f"Failed to setup workspace: {e}")
        raise RuntimeError(f"Failed to setup workspace: {e}") from e

    # Discover repository skills (from .cursorrules, agents.md, .hodor/skills/)
    skills = []
    try:
        skills = discover_skills(workspace)
        if skills:
            logger.info(f"Discovered {len(skills)} repository skill(s)")
        else:
            logger.debug("No repository skills found")
    except Exception as e:
        logger.warning(f"Failed to discover skills (continuing without skills): {e}")

    # Create OpenHands agent with repository skills
    try:
        agent = create_hodor_agent(
            model=model,
            temperature=temperature,
            reasoning_effort=reasoning_effort,
            verbose=verbose,
            llm_overrides=user_llm_params,
            skills=skills,
            lite_model=lite_model,
            enable_subagents=enable_subagents,
        )
    except Exception as e:
        logger.error(f"Failed to create OpenHands agent: {e}")
        if workspace and cleanup:
            cleanup_workspace(workspace)
        raise RuntimeError(f"Failed to create agent: {e}") from e

    mr_metadata = None
    if platform == "gitlab":
        try:
            mr_metadata = fetch_gitlab_mr_info(owner, repo, pr_number, host, include_comments=True)
        except GitLabAPIError as e:
            logger.warning(f"Failed to fetch GitLab metadata: {e}")
    elif platform == "github":
        try:
            github_raw = fetch_github_pr_info(owner, repo, pr_number)
            mr_metadata = normalize_github_metadata(github_raw)
        except GitHubAPIError as e:
            logger.warning(f"Failed to fetch GitHub metadata: {e}")

    # Build prompt
    try:
        prompt = build_pr_review_prompt(
            pr_url=pr_url,
            owner=owner,
            repo=repo,
            pr_number=str(pr_number),
            platform=platform,
            target_branch=target_branch,
            diff_base_sha=diff_base_sha,
            mr_metadata=mr_metadata,
            custom_instructions=custom_prompt,
            custom_prompt_file=prompt_file,
            output_format=output_format,
            enable_subagents=enable_subagents,
        )
    except Exception as e:
        logger.error(f"Failed to build prompt: {e}")
        if workspace and cleanup:
            cleanup_workspace(workspace)
        raise RuntimeError(f"Failed to build prompt: {e}") from e

    #  Event callback for monitoring agent progress
    def on_event(event: Any) -> None:
        """Callback for streaming agent events in verbose mode."""
        if not verbose:
            return

        event_type = type(event).__name__

        # Log LLM API calls (for detailed token/cost tracking)
        if isinstance(event, Event):
            # This captures raw LLM messages for detailed analysis
            # Useful for debugging prompt engineering or cost optimization
            logger.debug(f"ðŸ¤– LLM Event: {event_type}")

        # Log agent actions
        if hasattr(event, "action") and event.action:
            action_type = type(event.action).__name__
            if action_type == "ExecuteBashAction":
                logger.info(f"ðŸ”§ Executing: {event.action.command[:100]}")
            elif action_type == "FileEditAction":
                logger.info(f"âœï¸  Editing file: {getattr(event.action, 'file_path', 'unknown')}")
            elif action_type == "MessageAction":
                logger.info("ðŸ’¬ Agent thinking...")

        # Log observations (results)
        if hasattr(event, "observation") and event.observation:
            obs_type = type(event.observation).__name__
            if obs_type == "ExecuteBashObservation" and hasattr(event.observation, "exit_code"):
                exit_code = event.observation.exit_code
                status = "âœ“" if exit_code == 0 else "âœ—"
                logger.info(f"   {status} Exit code: {exit_code}")

        # Log errors
        if hasattr(event, "error") and event.error:
            logger.warning(f"âš ï¸  Error: {event.error}")

    import time

    start_time = time.time()

    try:
        logger.info("Creating OpenHands conversation...")
        # Use LocalWorkspace for better integration with OpenHands SDK
        workspace_obj = LocalWorkspace(working_dir=str(workspace))

        # Handle unlimited iterations (-1 -> very large number)
        iteration_limit = 1_000_000 if max_iterations == -1 else max_iterations

        # Register event callback for real-time monitoring if verbose
        # Use DelegationVisualizer to enable logging of worker agent actions
        conversation = Conversation(
            agent=agent,
            workspace=workspace_obj,
            callbacks=[on_event] if verbose else None,
            max_iteration_per_run=iteration_limit,
            visualizer=DelegationVisualizer(name="orchestrator"),
        )

        logger.info("Sending prompt to agent...")
        conversation.send_message(prompt)

        logger.info("Running agent review (this may take several minutes)...")

        conversation.run()

        logger.info("Extracting review from agent response...")
        review_content = get_agent_final_response(conversation.state.events)

        if not review_content:
            raise RuntimeError("Agent did not produce any review content")

        # Calculate review time
        review_time_seconds = time.time() - start_time
        review_time_str = f"{int(review_time_seconds // 60)}m {int(review_time_seconds % 60)}s"

        logger.info(f"Review complete ({len(review_content)} chars)")

        # Always print metrics (not just in verbose mode)
        # Access metrics via conversation.conversation_stats (SDK API)
        if hasattr(conversation, "conversation_stats"):
            try:
                combined = conversation.conversation_stats.get_combined_metrics()

                if combined and combined.accumulated_token_usage:
                    # Token usage breakdown from Metrics object
                    usage = combined.accumulated_token_usage
                    prompt_tokens = usage.prompt_tokens or 0
                    completion_tokens = usage.completion_tokens or 0
                    cache_read_tokens = usage.cache_read_tokens or 0
                    cache_write_tokens = usage.cache_write_tokens or 0
                    reasoning_tokens = usage.reasoning_tokens or 0
                    total_tokens = prompt_tokens + completion_tokens + cache_read_tokens + reasoning_tokens

                    # Cost estimate
                    cost = combined.accumulated_cost or 0

                    # Calculate cache hit rate
                    cache_hit_rate = 0
                    if cache_read_tokens > 0 and (prompt_tokens + cache_read_tokens) > 0:
                        cache_hit_rate = (cache_read_tokens / (prompt_tokens + cache_read_tokens)) * 100

                    # Print metrics (always, not just verbose)
                    print("\n" + "=" * 60)
                    print("ðŸ“Š Token Usage Metrics:")
                    print(f"  â€¢ Input tokens:       {prompt_tokens:,}")
                    print(f"  â€¢ Output tokens:      {completion_tokens:,}")
                    if cache_read_tokens > 0:
                        print(f"  â€¢ Cache hits:         {cache_read_tokens:,} ({cache_hit_rate:.1f}%)")
                    if reasoning_tokens > 0:
                        print(f"  â€¢ Reasoning tokens:   {reasoning_tokens:,}")
                    print(f"  â€¢ Total tokens:       {total_tokens:,}")
                    print(f"\nðŸ’° Cost Estimate:      ${cost:.4f}")
                    print(f"â±ï¸  Review Time:        {review_time_str}")
                    print("=" * 60 + "\n")

                    # Verbose mode: additional details
                    if verbose:
                        if cache_write_tokens > 0:
                            logger.info(f"  â€¢ Cache writes:       {cache_write_tokens:,}")
                        if combined.response_latencies:
                            avg_latency = sum(lat.latency for lat in combined.response_latencies) / len(combined.response_latencies)
                            logger.info(f"  â€¢ Avg API latency:    {avg_latency:.2f}s")
            except Exception as e:
                logger.warning(f"Failed to get metrics: {e}")

        return review_content

    except Exception as e:
        logger.error(f"Review failed: {e}")
        raise RuntimeError(f"Review failed: {e}") from e

    finally:
        # Reset terminal by draining leftover control-sequence replies
        _terminal_safety.restore_terminal_state()

        # Clean up workspace
        if workspace and cleanup:
            logger.info("Cleaning up workspace...")
            cleanup_workspace(workspace)
