"""Core agent for PR review using OpenHands SDK."""

import logging
import os
import subprocess
from pathlib import Path
from typing import Any, Literal
from urllib.parse import urlparse

from dotenv import load_dotenv
from openhands.sdk import Conversation
from openhands.sdk.conversation import get_agent_final_response

from .llm import create_hodor_agent, get_api_key
from .prompts.pr_review_prompt import build_pr_review_prompt
from .workspace import cleanup_workspace, setup_workspace

# Load environment variables
load_dotenv()

# Configure logging
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

Platform = Literal["github", "gitlab"]


def detect_platform(pr_url: str) -> Platform:
    """Detect the platform (GitHub or GitLab) from the PR URL."""
    parsed = urlparse(pr_url)
    hostname = parsed.hostname or ""

    # Check for GitLab-specific patterns first (works for both gitlab.com and self-hosted)
    if "/-/merge_requests/" in pr_url or "gitlab" in hostname:
        return "gitlab"
    # Check for GitHub-specific patterns
    elif "/pull/" in pr_url or "github" in hostname:
        return "github"
    else:
        logger.debug(f"Unknown platform for URL {pr_url}, defaulting to GitHub")
        return "github"


def parse_pr_url(pr_url: str) -> tuple[str, str, int]:
    """
    Parse PR/MR URL to extract owner, repo, and PR/MR number.

    Examples:
        GitHub: https://github.com/owner/repo/pull/123 -> ('owner', 'repo', 123)
        GitLab: https://gitlab.com/owner/repo/-/merge_requests/123 -> ('owner', 'repo', 123)
    """
    parsed = urlparse(pr_url)
    path_parts = [p for p in parsed.path.split("/") if p]

    # GitHub format: /owner/repo/pull/123
    if len(path_parts) >= 4 and path_parts[2] == "pull":
        owner = path_parts[0]
        repo = path_parts[1]
        pr_number = int(path_parts[3])
        return owner, repo, pr_number

    # GitLab format: /group/subgroup/repo/-/merge_requests/123
    elif "merge_requests" in path_parts:
        mr_index = path_parts.index("merge_requests")
        if mr_index < 2 or mr_index + 1 >= len(path_parts):
            raise ValueError(f"Invalid GitLab MR URL format: {pr_url}. Expected .../-/merge_requests/<number>")
        if path_parts[mr_index - 1] != "-":
            raise ValueError(f"Invalid GitLab MR URL format: {pr_url}. Missing '/-/' segment before merge_requests.")

        repo = path_parts[mr_index - 2]
        owner_parts = path_parts[: mr_index - 2]
        owner = "/".join(owner_parts) if owner_parts else path_parts[0]
        pr_number = int(path_parts[mr_index + 1])
        return owner, repo, pr_number

    else:
        raise ValueError(
            f"Invalid PR/MR URL format: {pr_url}. Expected GitHub pull request or GitLab merge request URL."
        )


def post_review_comment(
    pr_url: str,
    review_text: str,
    model: str | None = None,
) -> dict[str, Any]:
    """
    Post a review comment on a GitHub PR or GitLab MR using CLI tools.

    Args:
        pr_url: URL of the pull request or merge request
        review_text: The review text to post as a comment
        model: LLM model used for the review (optional, for transparency)

    Returns:
        Dictionary with comment posting result
    """
    platform = detect_platform(pr_url)
    logger.info(f"Posting comment to {platform} PR/MR: {pr_url}")

    try:
        owner, repo, pr_number = parse_pr_url(pr_url)
    except ValueError as e:
        return {"success": False, "error": str(e)}

    # Append model information to review text for transparency
    if model:
        review_text_with_footer = f"{review_text}\n\n---\n\n*Review generated by Hodor using `{model}`*"
    else:
        review_text_with_footer = review_text

    try:
        if platform == "github":
            # Use gh CLI to post comment
            subprocess.run(
                [
                    "gh",
                    "pr",
                    "review",
                    str(pr_number),
                    "--repo",
                    f"{owner}/{repo}",
                    "--comment",
                    "--body",
                    review_text_with_footer,
                ],
                check=True,
                capture_output=True,
                text=True,
            )
            logger.info(f"Successfully posted review to GitHub PR #{pr_number}")
            return {"success": True, "platform": "github", "pr_number": pr_number}

        elif platform == "gitlab":
            # Use glab CLI to post comment
            # Note: glab needs to be authenticated for the right GitLab instance
            subprocess.run(
                [
                    "glab",
                    "mr",
                    "note",
                    str(pr_number),
                    "--message",
                    review_text_with_footer,
                ],
                check=True,
                capture_output=True,
                text=True,
                env={**os.environ},  # Pass GITLAB_HOST if set
            )
            logger.info(f"Successfully posted review to GitLab MR !{pr_number}")
            return {"success": True, "platform": "gitlab", "mr_number": pr_number}

        else:
            return {"success": False, "error": f"Unsupported platform: {platform}"}

    except subprocess.CalledProcessError as e:
        logger.error(f"Failed to post comment: {e}")
        logger.error(f"Command output: {e.stderr if hasattr(e, 'stderr') else 'N/A'}")
        return {"success": False, "error": str(e)}
    except Exception as e:
        logger.error(f"Error posting comment: {str(e)}")
        return {"success": False, "error": str(e)}


def review_pr(
    pr_url: str,
    model: str = "anthropic/claude-sonnet-4-5-20250929",
    temperature: float | None = None,
    reasoning_effort: str | None = None,
    custom_prompt: str | None = None,
    prompt_file: Path | None = None,
    user_llm_params: dict[str, Any] | None = None,
    verbose: bool = False,
    cleanup: bool = True,
    workspace_dir: Path | None = None,
) -> str:
    """
    Review a pull request using OpenHands agent with bash tools.

    Args:
        pr_url: URL of the pull request or merge request
        model: LLM model name (default: Claude Sonnet 4.5)
        temperature: Sampling temperature (if None, auto-selected)
        reasoning_effort: For reasoning models: "low", "medium", or "high"
        custom_prompt: Optional custom prompt text (inline)
        prompt_file: Optional path to custom prompt file
        user_llm_params: Additional LLM parameters
        verbose: Enable verbose logging
        cleanup: Clean up workspace after review (default: True)
        workspace_dir: Directory to use for workspace (if None, creates temp dir). Reuses if same repo.

    Returns:
        Review text as markdown string

    Raises:
        ValueError: If URL is invalid
        RuntimeError: If review fails
    """
    logger.info(f"Starting PR review for: {pr_url}")

    # Parse PR URL
    try:
        owner, repo, pr_number = parse_pr_url(pr_url)
        platform = detect_platform(pr_url)
    except ValueError as e:
        logger.error(f"Invalid PR URL: {e}")
        raise

    logger.info(f"Platform: {platform}, Repo: {owner}/{repo}, PR: {pr_number}")

    # Create OpenHands agent
    try:
        agent = create_hodor_agent(
            model=model,
            temperature=temperature,
            reasoning_effort=reasoning_effort,
            verbose=verbose,
            llm_overrides=user_llm_params,
        )
    except Exception as e:
        logger.error(f"Failed to create OpenHands agent: {e}")
        raise RuntimeError(f"Failed to create agent: {e}") from e

    # Setup workspace (clone repo and checkout PR branch)
    workspace = None
    try:
        workspace = setup_workspace(
            platform=platform,
            owner=owner,
            repo=repo,
            pr_number=str(pr_number),
            working_dir=workspace_dir,
            reuse=workspace_dir is not None,  # Only reuse if user specified a workspace dir
        )
        logger.info(f"Workspace ready: {workspace}")
    except Exception as e:
        logger.error(f"Failed to setup workspace: {e}")
        raise RuntimeError(f"Failed to setup workspace: {e}") from e

    # Build prompt
    try:
        prompt = build_pr_review_prompt(
            pr_url=pr_url,
            owner=owner,
            repo=repo,
            pr_number=str(pr_number),
            platform=platform,
            custom_instructions=custom_prompt,
            custom_prompt_file=prompt_file,
        )
    except Exception as e:
        logger.error(f"Failed to build prompt: {e}")
        if workspace and cleanup:
            cleanup_workspace(workspace)
        raise RuntimeError(f"Failed to build prompt: {e}") from e

    # Create conversation and run agent
    # NOTE: Temporary workaround for NixOS compatibility (where bash is not at /bin/bash)
    # TODO: Remove this once OpenHands SDK adds bash_path parameter to SubprocessTerminal
    #       See https://github.com/OpenHands/agent-sdk/issues/TBD
    import shutil

    bash_path = shutil.which("bash") or "/bin/bash"
    if bash_path != "/bin/bash":
        logger.debug(f"Patching SubprocessTerminal for NixOS (bash at {bash_path})")
        try:
            from openhands.tools.terminal.terminal import subprocess_terminal

            original_initialize = subprocess_terminal.SubprocessTerminal.initialize

            def patched_initialize(self):
                if self._initialized:
                    return
                import fcntl
                import pty
                import threading
                import uuid

                env = os.environ.copy()
                env["PS1"] = self.PS1
                env["PS2"] = ""
                env["TERM"] = "xterm-256color"
                bash_cmd = [bash_path, "-i"]  # Use discovered bash instead of /bin/bash
                master_fd, slave_fd = pty.openpty()

                logger.debug("Initializing PTY with: %s", " ".join(bash_cmd))
                try:
                    self.process = subprocess.Popen(
                        bash_cmd,
                        stdin=slave_fd,
                        stdout=slave_fd,
                        stderr=slave_fd,
                        cwd=self.work_dir,
                        env=env,
                        text=False,
                        bufsize=0,
                        preexec_fn=os.setsid,  # New process group
                        close_fds=True,
                    )
                finally:
                    try:
                        os.close(slave_fd)
                    except Exception:
                        pass

                self._pty_master_fd = master_fd

                # Set master FD non-blocking
                flags = fcntl.fcntl(self._pty_master_fd, fcntl.F_GETFL)
                fcntl.fcntl(self._pty_master_fd, fcntl.F_SETFL, flags | os.O_NONBLOCK)

                # Start output reader thread
                self.reader_thread = threading.Thread(
                    target=self._read_output_continuously_pty, daemon=True
                )
                self.reader_thread.start()
                self._initialized = True

                # Deterministic readiness check with sentinel
                sentinel = f"__OH_READY_{uuid.uuid4().hex}__"
                from openhands.tools.terminal.terminal.subprocess_terminal import ENTER

                init_cmd = (
                    f"export PROMPT_COMMAND='export PS1=\"{self.PS1}\"'; "
                    f'export PS2=""; '
                    f'printf "{sentinel}"'
                ).encode("utf-8", "ignore")

                self._write_pty(init_cmd + ENTER)
                if not self._wait_for_output(sentinel, timeout=8.0):
                    raise RuntimeError("PTY did not become ready within timeout")

            subprocess_terminal.SubprocessTerminal.initialize = patched_initialize
            logger.info(f"Applied NixOS patch (bash: {bash_path})")
        except Exception as e:
            logger.warning(f"Failed to patch SubprocessTerminal: {e}")

    #  Event callback for monitoring agent progress
    def on_event(event: Any) -> None:
        """Callback for streaming agent events in verbose mode."""
        if not verbose:
            return

        event_type = type(event).__name__

        # Log LLM API calls (for detailed token/cost tracking)
        from openhands.events.event import LLMConvertibleEvent

        if isinstance(event, LLMConvertibleEvent):
            # This captures raw LLM messages for detailed analysis
            # Useful for debugging prompt engineering or cost optimization
            logger.debug(f"ðŸ¤– LLM Event: {event_type}")

        # Log agent actions
        if hasattr(event, "action") and event.action:
            action_type = type(event.action).__name__
            if action_type == "ExecuteBashAction":
                logger.info(f"ðŸ”§ Executing: {event.action.command[:100]}")
            elif action_type == "FileEditAction":
                logger.info(f"âœï¸  Editing file: {getattr(event.action, 'file_path', 'unknown')}")
            elif action_type == "MessageAction":
                logger.info(f"ðŸ’¬ Agent thinking...")

        # Log observations (results)
        if hasattr(event, "observation") and event.observation:
            obs_type = type(event.observation).__name__
            if obs_type == "ExecuteBashObservation" and hasattr(event.observation, "exit_code"):
                exit_code = event.observation.exit_code
                status = "âœ“" if exit_code == 0 else "âœ—"
                logger.info(f"   {status} Exit code: {exit_code}")

        # Log errors
        if hasattr(event, "error") and event.error:
            logger.warning(f"âš ï¸  Error: {event.error}")

    import time

    start_time = time.time()

    try:
        logger.info("Creating OpenHands conversation...")
        conversation = Conversation(agent=agent, workspace=str(workspace))

        logger.info("Sending prompt to agent...")
        conversation.send_message(prompt)

        logger.info("Running agent review (this may take several minutes)...")

        # Run with event streaming if verbose
        if verbose:
            # Register event callback for real-time monitoring
            conversation._state.on_event = on_event

        conversation.run()

        logger.info("Extracting review from agent response...")
        review_content = get_agent_final_response(conversation.state.events)

        if not review_content:
            raise RuntimeError("Agent did not produce any review content")

        # Calculate review time
        review_time_seconds = time.time() - start_time
        review_time_str = f"{int(review_time_seconds // 60)}m {int(review_time_seconds % 60)}s"

        logger.info(f"Review complete ({len(review_content)} chars)")

        # Always print metrics (not just in verbose mode)
        if hasattr(conversation, "conversation_stats"):
            stats = conversation.conversation_stats
            combined = stats.get_combined_metrics()

            if combined:
                # Token usage breakdown
                usage = combined.get("accumulated_token_usage", {})
                prompt_tokens = usage.get("prompt_tokens", 0)
                completion_tokens = usage.get("completion_tokens", 0)
                cache_read_tokens = usage.get("cache_read_input_tokens", 0)
                cache_write_tokens = usage.get("cache_creation_input_tokens", 0)
                reasoning_tokens = usage.get("reasoning_tokens", 0)
                total_tokens = usage.get("total_tokens", 0)

                # Cost estimate
                cost = combined.get("accumulated_cost", 0)

                # Calculate cache hit rate
                cache_hit_rate = 0
                if cache_read_tokens > 0 and (prompt_tokens + cache_read_tokens) > 0:
                    cache_hit_rate = (cache_read_tokens / (prompt_tokens + cache_read_tokens)) * 100

                # Print metrics (always, not just verbose)
                print("\n" + "=" * 60)
                print("ðŸ“Š Token Usage Metrics:")
                print(f"  â€¢ Input tokens:       {prompt_tokens:,}")
                print(f"  â€¢ Output tokens:      {completion_tokens:,}")
                if cache_read_tokens > 0:
                    print(f"  â€¢ Cache hits:         {cache_read_tokens:,} ({cache_hit_rate:.1f}%)")
                if reasoning_tokens > 0:
                    print(f"  â€¢ Reasoning tokens:   {reasoning_tokens:,}")
                print(f"  â€¢ Total tokens:       {total_tokens:,}")
                print(f"\nðŸ’° Cost Estimate:      ${cost:.4f}")
                print(f"â±ï¸  Review Time:        {review_time_str}")
                print("=" * 60 + "\n")

                # Verbose mode: additional details
                if verbose:
                    if cache_write_tokens > 0:
                        logger.info(f"  â€¢ Cache writes:       {cache_write_tokens:,}")
                    latencies = combined.get("response_latencies", [])
                    if latencies:
                        avg_latency = sum(latencies) / len(latencies)
                        logger.info(f"  â€¢ Avg API latency:    {avg_latency:.2f}s")

        return review_content

    except Exception as e:
        logger.error(f"Review failed: {e}")
        raise RuntimeError(f"Review failed: {e}") from e

    finally:
        # Reset terminal to prevent corruption from PTY
        # The subprocess terminal can leave escape sequences that corrupt the parent shell
        try:
            import sys
            import subprocess as sp

            # Reset terminal attributes if we're in a TTY (without clearing screen)
            if sys.stdin.isatty():
                # Use stty sane to reset terminal to sensible defaults
                sp.run(
                    ["stty", "sane"],
                    stdin=sys.stdin,
                    stdout=sp.DEVNULL,
                    stderr=sp.DEVNULL,
                    check=False,
                )
        except Exception:
            pass  # Silently ignore if terminal reset fails

        # Clean up workspace
        if workspace and cleanup:
            logger.info("Cleaning up workspace...")
            cleanup_workspace(workspace)
